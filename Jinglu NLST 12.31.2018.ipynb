{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge pydotplus -y\n",
    "#!conda install -c conda-forge python-graphviz -y\n",
    "# Notice: You might need to uncomment and install the pydotplus and graphviz libraries if you have not installed these before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get datasets from local machine into Jupyter Pandas dataframe, check shape\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1: patient dataset, df_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jing/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (99,248,249) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53452, 324)\n",
      "(2150, 48)\n",
      "(177487, 12)\n"
     ]
    }
   ],
   "source": [
    "df_pt_full = pd.read_csv(\"/Users/jing/Downloads/package-nlst-229-2018.11.17/participant.data.d100517.csv\")\n",
    "df_lc = pd.read_csv('/Users/jing/Downloads/package-nlst-229-2018.11.17/Lung Cancer/lung_cancer.data.d100517.csv')\n",
    "df_sctabn_full = pd.read_csv('/Users/jing/Downloads/package-nlst-229-2018.11.17/Spiral CT Abnormalities/sct_abnormalities.data.d100517.csv')\n",
    "print(df_pt_full.shape)\n",
    "print(df_lc.shape)\n",
    "print(df_sctabn_full.shape)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of this datset: (6379, 324)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of patient</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Number of patient\n",
       "conflc                   \n",
       "2                    5290\n",
       "1                    1089"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Trim dataset: Selecting only the people being CT scanned, and those has a record of Cancer vs. Non Cancer\n",
    "## Select only CT scan\n",
    "df_pt_ct = df_pt_full.loc[df_pt_full['rndgroup'] == 1]\n",
    "## Select only people that has a record of cancer vs non cancer\n",
    "df_pt = df_pt_ct.loc[df_pt_ct['conflc'].isin([1,2])]\n",
    "print('Shape of this datset:', df_pt.shape)\n",
    "## This will trim the patient dataset df_pt into 6379 rows, from the original of 53452 rows\n",
    "## summarize the cancer/Non-cancer count in pt dataset\n",
    "conflc_counts = df_pt['conflc'].value_counts().to_frame()\n",
    "conflc_counts.rename(columns={'conflc': 'Number of patient'}, inplace=True)\n",
    "conflc_counts.index.name = 'conflc'\n",
    "conflc_counts\n",
    "# 1089 with cancer, and 5290 without cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3 -Sct_abnormality: df_sctabn and its trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81356, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Select in abnormality only the rows that has sct_ab_desc in 51,52,53,62\n",
    "\n",
    "df_abn_bigsmallnomany = df_sctabn_full.loc[df_sctabn_full['sct_ab_desc'].isin([51,52,53,62])]\n",
    "\n",
    "## select columns in abnormality dataset that's only useful for the research\n",
    "df_abn_useful = df_abn_bigsmallnomany[['dataset_version','pid','sct_ab_desc','sct_ab_num', 'sct_epi_loc',\n",
    "                                       'sct_long_dia','sct_slice_num','study_yr']] \n",
    "df_abn_useful.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculation of largest nodule, total nodule size, number of nodule\n",
    "df_abn_agg = df_abn_useful[['pid','sct_long_dia','study_yr']]\n",
    "df_abn_nodulesum =    df_abn_agg.groupby(['pid'], as_index = False).sum()\n",
    "df_abn_nodulemax =    df_abn_agg.groupby(['pid'], as_index = False).max()\n",
    "df_abn_nodulecounts = df_abn_agg.groupby(['pid'], as_index = False).count()\n",
    "## 81,356 record will become 19,116 after picking the sum/max/count with one pid only appear once\n",
    "## Renaming the 3 data frames with each of their 'sct_long_dia' columns to indicate it's a sum, max, or a count\n",
    "df_abn_nodulesum.rename(columns={'sct_long_dia': 'sct_long_dia_sum'}, inplace=True)\n",
    "df_abn_nodulemax.rename(columns={'sct_long_dia': 'sct_long_dia_max'}, inplace=True)\n",
    "df_abn_nodulecounts.rename(columns={'sct_long_dia': 'sct_long_dia_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: I'm only looking at nodules that are > 4mm. But I did not look at small nodules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining #1 df_pt with #3 df_sct_abn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6379, 330)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 辣鸡代码效率可以提高\n",
    "df_pt_abn_sum = pd.merge(df_pt, df_abn_nodulesum, how='left', left_on='pid', right_on='pid')\n",
    "df_pt_abn_sum_max = pd.merge(df_pt_abn_sum, df_abn_nodulemax, how='left', left_on='pid', right_on='pid')\n",
    "df_pt_abn_sum_max_counts = pd.merge(df_pt_abn_sum_max, df_abn_nodulecounts, how='left', left_on='pid', right_on='pid')\n",
    "df_pt_abn_sum_max_counts.shape\n",
    "## From here, the pt dataset is combined with abnormal dataset, with nodules size info, that contains max/sum/counts\n",
    "## The N match exactly what has been discussed, which is 6379 patients has a record of Cancer/No Cancer\n",
    "## Surprisingly, and happlily, the N=6379 has not been decreased by joining the abn dataset with sct_ab_desc in (51, 52, 53, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling Table 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the final dataset is:  (6379, 34)\n",
      "How many missing values in each columns? \n",
      " pid                     0\n",
      "age                     0\n",
      "gender                  0\n",
      "smokelive              34\n",
      "race                    0\n",
      "pkyr                    0\n",
      "smokework              58\n",
      "famfather             160\n",
      "fammother             139\n",
      "anyscr_has_nodule      15\n",
      "conflc                  0\n",
      "sct_long_dia_sum      171\n",
      "sct_long_dia_max      411\n",
      "sct_long_dia_count    171\n",
      "study_yr              171\n",
      "diagcopd               10\n",
      "wrkasbe                 7\n",
      "wrkbaki                 6\n",
      "wrkbutc                 6\n",
      "wrkchem                 6\n",
      "wrkcoal                 6\n",
      "wrkcott                 7\n",
      "wrkfarm                 7\n",
      "wrkfire                 6\n",
      "wrkflou                 6\n",
      "wrkfoun                 6\n",
      "wrkhard                 6\n",
      "wrkpain                 6\n",
      "wrksand                 6\n",
      "wrkweld                 6\n",
      "cigsmok                 0\n",
      "diagadas                8\n",
      "smokeday                0\n",
      "marital                 6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Building table 1 for the first model(this we only use dataset pt, and CT, but not lc)\n",
    "## Based on literature review of Jinglu's code, and meeting discussions on Dec. 6, \n",
    "## We select following variables for analysis: literature review, occupational hazard, athma, #cigar smoked, current smoker\n",
    "\n",
    "df_pt_abn = df_pt_abn_sum_max_counts[['pid', 'age','gender', 'smokelive','race','pkyr','smokework',\n",
    "                                      'famfather','fammother','anyscr_has_nodule','conflc', 'sct_long_dia_sum'\n",
    "                                      ,'sct_long_dia_max', 'sct_long_dia_count','study_yr','diagcopd'\n",
    "                                      # Add in occupational hazard\n",
    "                                      ,'wrkasbe','wrkbaki','wrkbutc','wrkchem','wrkcoal','wrkcott','wrkfarm'\n",
    "                                      ,'wrkfire', 'wrkflou', 'wrkfoun','wrkhard','wrkpain','wrksand','wrkweld'\n",
    "                                      # Add in other: cigsmok current or former(quit 15 years)\n",
    "                                      , 'cigsmok', 'diagadas','smokeday','marital']]\n",
    "\n",
    "## Did Jinglu use max, sum or count??? Which COPD is important?? \n",
    "print('The shape of the final dataset is: ',df_pt_abn.shape)\n",
    "\n",
    "print('How many missing values in each columns?', '\\n', df_pt_abn.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/jing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>smokelive</th>\n",
       "      <th>race</th>\n",
       "      <th>pkyr</th>\n",
       "      <th>smokework</th>\n",
       "      <th>famfather</th>\n",
       "      <th>fammother</th>\n",
       "      <th>anyscr_has_nodule</th>\n",
       "      <th>conflc</th>\n",
       "      <th>sct_long_dia_sum</th>\n",
       "      <th>sct_long_dia_max</th>\n",
       "      <th>sct_long_dia_count</th>\n",
       "      <th>study_yr</th>\n",
       "      <th>diagcopd</th>\n",
       "      <th>wrkasbe</th>\n",
       "      <th>wrkbaki</th>\n",
       "      <th>wrkbutc</th>\n",
       "      <th>wrkchem</th>\n",
       "      <th>wrkcoal</th>\n",
       "      <th>wrkcott</th>\n",
       "      <th>wrkfarm</th>\n",
       "      <th>wrkfire</th>\n",
       "      <th>wrkflou</th>\n",
       "      <th>wrkfoun</th>\n",
       "      <th>wrkhard</th>\n",
       "      <th>wrkpain</th>\n",
       "      <th>wrksand</th>\n",
       "      <th>wrkweld</th>\n",
       "      <th>cigsmok</th>\n",
       "      <th>diagadas</th>\n",
       "      <th>smokeday</th>\n",
       "      <th>marital</th>\n",
       "      <th>wrk_total</th>\n",
       "      <th>wrk_or_not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100004</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100012</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100019</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100026</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100035</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid  age  gender  smokelive  race  pkyr  smokework  famfather  \\\n",
       "0  100004   60       1        0.0     1  34.0        1.0        0.0   \n",
       "1  100012   61       2        1.0     1  37.0        1.0        NaN   \n",
       "2  100019   61       1        0.0     1  78.0        1.0        0.0   \n",
       "3  100026   57       1        1.0     1  61.5        1.0        0.0   \n",
       "4  100035   55       2        1.0     1  38.0        1.0        0.0   \n",
       "\n",
       "   fammother  anyscr_has_nodule  conflc  sct_long_dia_sum  sct_long_dia_max  \\\n",
       "0        0.0                1.0       2               8.0               4.0   \n",
       "1        NaN                1.0       1              23.0              15.0   \n",
       "2        0.0                1.0       2              14.0              14.0   \n",
       "3        0.0                1.0       2              17.0               5.0   \n",
       "4        0.0                1.0       2              15.0               5.0   \n",
       "\n",
       "   sct_long_dia_count  study_yr  diagcopd  wrkasbe  wrkbaki  wrkbutc  wrkchem  \\\n",
       "0                 2.0       3.0       0.0      1.0      0.0      0.0      0.0   \n",
       "1                 2.0       2.0       0.0      0.0      0.0      0.0      0.0   \n",
       "2                 1.0       2.0       0.0      0.0      0.0      0.0      0.0   \n",
       "3                 4.0      10.0       0.0      0.0      0.0      0.0      0.0   \n",
       "4                 3.0       5.0       0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   wrkcoal  wrkcott  wrkfarm  wrkfire  wrkflou  wrkfoun  wrkhard  wrkpain  \\\n",
       "0      0.0      0.0      1.0      1.0      1.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   wrksand  wrkweld  cigsmok  diagadas  smokeday  marital  wrk_total  \\\n",
       "0      0.0      1.0        0       0.0        40      2.0        4.0   \n",
       "1      0.0      0.0        1       0.0        20      2.0        0.0   \n",
       "2      0.0      0.0        0       0.0        40      2.0        0.0   \n",
       "3      0.0      0.0        0       0.0        30      2.0        1.0   \n",
       "4      0.0      0.0        1       0.0        20      2.0        0.0   \n",
       "\n",
       "   wrk_or_not  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Dec. 31, Collaps multiple variables into one\n",
    "## On Dec. 31, sum up the value of 13 occupational hazard variables into one, except for wrkasbe for asbestos is particularly cancer-causing than other variables\n",
    "df_pt_abn['wrk_total']= df_pt_abn.loc[:, ['wrkbaki','wrkbutc','wrkchem','wrkcoal','wrkcott','wrkfarm',\n",
    "           'wrkfire', 'wrkflou', 'wrkfoun','wrkhard','wrkpain','wrksand',\n",
    "           'wrkweld']].sum(axis=1)\n",
    "## I not only wanted the sum, but also need a binary variable indicating 'have worked in hazardous job or not'\n",
    "df_pt_abn['wrk_or_not'] = np.where(df_pt_abn['wrk_total']!=0, 1, 0)\n",
    "\n",
    "df_pt_abn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6379, 37)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Create new variable for marital_status\n",
    "## Define how to label\n",
    "def label_marital (row):\n",
    "    if row['marital'] == 1:\n",
    "        return '1'\n",
    "    if row['marital'] == 2:\n",
    "        return '2'\n",
    "    if row['marital'] in([3,4,5]):\n",
    "        return '3'\n",
    "    else: \n",
    "        return '0'\n",
    "df_pt_abn['Marital_Status'] = df_pt_abn.apply (lambda row: label_marital (row),axis=1)\n",
    "df_pt_abn.shape\n",
    "# M=\"Missing\"\n",
    "# 1=\"Never married\"\n",
    "# 2=\"Married or living as married\" 3=\"Widowed\"\n",
    "# 4=\"Separated\"\n",
    "# 5=\"Divorced\"\n",
    "# 7=\"Participant refused to answer\" 9=\"Not Ascertained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5757, 37)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop all rows that has missing data\n",
    "df_pt_abn = df_pt_abn.dropna()\n",
    "df_pt_abn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pid                     int32\n",
       "age                     int32\n",
       "gender                 object\n",
       "smokelive              object\n",
       "race                   object\n",
       "pkyr                  float32\n",
       "smokework              object\n",
       "famfather              object\n",
       "fammother              object\n",
       "anyscr_has_nodule      object\n",
       "conflc                  int64\n",
       "sct_long_dia_sum      float32\n",
       "sct_long_dia_max      float32\n",
       "sct_long_dia_count    float32\n",
       "study_yr               object\n",
       "diagcopd               object\n",
       "wrkasbe               float64\n",
       "wrkbaki               float64\n",
       "wrkbutc               float64\n",
       "wrkchem               float64\n",
       "wrkcoal               float64\n",
       "wrkcott               float64\n",
       "wrkfarm               float64\n",
       "wrkfire               float64\n",
       "wrkflou               float64\n",
       "wrkfoun               float64\n",
       "wrkhard               float64\n",
       "wrkpain               float64\n",
       "wrksand               float64\n",
       "wrkweld               float64\n",
       "cigsmok                 int64\n",
       "diagadas              float64\n",
       "smokeday                int64\n",
       "marital               float64\n",
       "wrk_total             float64\n",
       "wrk_or_not              int64\n",
       "Marital_Status         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Correcting Data Types for modeling purpose\n",
    "df_pt_abn['gender'] = df_pt_abn['gender'].astype('object')\n",
    "df_pt_abn['smokelive'] = df_pt_abn['smokelive'].astype('object')\n",
    "df_pt_abn['race'] = df_pt_abn['race'].astype('object')\n",
    "df_pt_abn['smokework'] = df_pt_abn['smokework'].astype('object')\n",
    "df_pt_abn['famfather'] = df_pt_abn['famfather'].astype('object')\n",
    "df_pt_abn['fammother'] = df_pt_abn['fammother'].astype('object')\n",
    "df_pt_abn['anyscr_has_nodule'] = df_pt_abn['anyscr_has_nodule'].astype('object')\n",
    "df_pt_abn['conflc'] = df_pt_abn['conflc'].astype('int')\n",
    "df_pt_abn['study_yr'] = df_pt_abn['study_yr'].astype('object')\n",
    "df_pt_abn['diagcopd'] = df_pt_abn['diagcopd'].astype('object')\n",
    "## COPD is explained Dec.6\n",
    "df_pt_abn['pkyr'] = df_pt_abn['pkyr'].astype('float32')\n",
    "df_pt_abn['sct_long_dia_sum'] = df_pt_abn['sct_long_dia_sum'].astype('float32')\n",
    "df_pt_abn['sct_long_dia_max'] = df_pt_abn['sct_long_dia_max'].astype('float32')\n",
    "df_pt_abn['sct_long_dia_count'] = df_pt_abn['sct_long_dia_count'].astype('float32')\n",
    "df_pt_abn['pid'] = df_pt_abn['pid'].astype('int32')\n",
    "df_pt_abn['age'] = df_pt_abn['age'].astype('int32')\n",
    "df_pt_abn.dtypes\n",
    "## Need to also do this for new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    3459\n",
      "2    2298\n",
      "Name: gender, dtype: int64\n",
      "                              \n",
      "1.0    5046\n",
      "0.0     711\n",
      "Name: smokelive, dtype: int64\n",
      "                              \n",
      "1     5302\n",
      "2      182\n",
      "3      149\n",
      "6       78\n",
      "5       24\n",
      "4       15\n",
      "7        4\n",
      "99       1\n",
      "98       1\n",
      "96       1\n",
      "Name: race, dtype: int64\n",
      "                              \n",
      "1.0    4877\n",
      "0.0     880\n",
      "Name: smokework, dtype: int64\n",
      "                              \n",
      "0.0    5167\n",
      "1.0     590\n",
      "Name: famfather, dtype: int64\n",
      "                              \n",
      "0.0    5418\n",
      "1.0     339\n",
      "Name: fammother, dtype: int64\n",
      "                              \n",
      "1.0    5757\n",
      "Name: anyscr_has_nodule, dtype: int64\n",
      "                              \n",
      "2    4959\n",
      "1     798\n",
      "Name: conflc, dtype: int64\n",
      "                              \n",
      "5.0      1079\n",
      "6.0       855\n",
      "4.0       721\n",
      "7.0       648\n",
      "8.0       482\n",
      "10.0      305\n",
      "9.0       302\n",
      "11.0      182\n",
      "12.0      163\n",
      "13.0      136\n",
      "15.0      108\n",
      "14.0       96\n",
      "17.0       79\n",
      "16.0       75\n",
      "18.0       66\n",
      "20.0       53\n",
      "19.0       46\n",
      "24.0       30\n",
      "28.0       28\n",
      "23.0       26\n",
      "22.0       25\n",
      "21.0       24\n",
      "30.0       23\n",
      "27.0       18\n",
      "25.0       18\n",
      "29.0       16\n",
      "26.0       14\n",
      "33.0       12\n",
      "35.0       12\n",
      "34.0       10\n",
      "         ... \n",
      "50.0        5\n",
      "46.0        4\n",
      "39.0        4\n",
      "49.0        3\n",
      "60.0        3\n",
      "58.0        3\n",
      "41.0        3\n",
      "61.0        2\n",
      "80.0        2\n",
      "54.0        2\n",
      "42.0        2\n",
      "3.0         2\n",
      "52.0        2\n",
      "45.0        1\n",
      "47.0        1\n",
      "90.0        1\n",
      "55.0        1\n",
      "48.0        1\n",
      "51.0        1\n",
      "53.0        1\n",
      "78.0        1\n",
      "130.0       1\n",
      "74.0        1\n",
      "43.0        1\n",
      "66.0        1\n",
      "65.0        1\n",
      "73.0        1\n",
      "56.0        1\n",
      "44.0        1\n",
      "110.0       1\n",
      "Name: sct_long_dia_max, Length: 66, dtype: int64\n",
      "                              \n",
      "1.0     1531\n",
      "3.0     1273\n",
      "2.0     1051\n",
      "4.0      503\n",
      "6.0      372\n",
      "5.0      323\n",
      "7.0      152\n",
      "8.0      134\n",
      "9.0      122\n",
      "10.0      70\n",
      "12.0      67\n",
      "11.0      50\n",
      "13.0      30\n",
      "15.0      23\n",
      "14.0      19\n",
      "16.0      19\n",
      "17.0      10\n",
      "18.0       7\n",
      "28.0       1\n",
      "Name: sct_long_dia_count, dtype: int64\n",
      "                              \n",
      "3.0     847\n",
      "4.0     713\n",
      "2.0     651\n",
      "1.0     621\n",
      "6.0     569\n",
      "5.0     568\n",
      "7.0     402\n",
      "8.0     314\n",
      "9.0     243\n",
      "10.0    219\n",
      "11.0    146\n",
      "12.0    133\n",
      "13.0     89\n",
      "14.0     68\n",
      "15.0     61\n",
      "16.0     42\n",
      "17.0     35\n",
      "18.0     33\n",
      "20.0      1\n",
      "19.0      1\n",
      "28.0      1\n",
      "Name: study_yr, dtype: int64\n",
      "                              \n",
      "0.0    5432\n",
      "1.0     325\n",
      "Name: diagcopd, dtype: int64\n",
      "                              \n",
      "0.0    5481\n",
      "1.0     276\n",
      "Name: wrkasbe, dtype: int64\n",
      "                              \n",
      "0.0     4143\n",
      "1.0     1033\n",
      "2.0      327\n",
      "3.0      157\n",
      "4.0       68\n",
      "5.0       20\n",
      "6.0        4\n",
      "7.0        3\n",
      "11.0       1\n",
      "8.0        1\n",
      "Name: wrk_total, dtype: int64\n",
      "                              \n",
      "0    4143\n",
      "1    1614\n",
      "Name: wrk_or_not, dtype: int64\n",
      "                              \n",
      "0    2998\n",
      "1    2759\n",
      "Name: cigsmok, dtype: int64\n",
      "                              \n",
      "0.0    5398\n",
      "1.0     359\n",
      "Name: diagadas, dtype: int64\n",
      "                              \n",
      "20     2392\n",
      "30     1274\n",
      "40      967\n",
      "25      298\n",
      "60      212\n",
      "15      173\n",
      "50      141\n",
      "35       72\n",
      "18       60\n",
      "80       27\n",
      "45       22\n",
      "17       14\n",
      "70       13\n",
      "22       11\n",
      "16       10\n",
      "23        9\n",
      "100       8\n",
      "28        5\n",
      "38        5\n",
      "55        5\n",
      "24        5\n",
      "27        5\n",
      "19        4\n",
      "12        4\n",
      "13        3\n",
      "90        2\n",
      "11        2\n",
      "32        1\n",
      "36        1\n",
      "56        1\n",
      "87        1\n",
      "21        1\n",
      "33        1\n",
      "37        1\n",
      "49        1\n",
      "14        1\n",
      "26        1\n",
      "42        1\n",
      "82        1\n",
      "39        1\n",
      "57        1\n",
      "Name: smokeday, dtype: int64\n",
      "                              \n",
      "2    4055\n",
      "3    1483\n",
      "1     217\n",
      "0       2\n",
      "Name: Marital_Status, dtype: int64\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "a = list(['gender', 'smokelive','race','smokework','famfather','fammother','anyscr_has_nodule','conflc',\n",
    "          'sct_long_dia_max', 'sct_long_dia_count','study_yr','diagcopd',\n",
    "          ## Here I added in three varibles created in Dec. 31 that are occupational hazard related\n",
    "          'wrkasbe','wrk_total','wrk_or_not',\n",
    "          'cigsmok', 'diagadas','smokeday','Marital_Status'\n",
    "         ])\n",
    "for column in a:\n",
    "    print(df_pt_abn[column].value_counts())\n",
    "    print('                              ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = '/Users/dahailiu/Downloads/20181127_1226.csv'\n",
    "#df_pt_abn.to_csv(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7cd0e99cea10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import tree\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Here is how Jinglu measured 3 times of feature importance, which will inform feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "df_pt_abn = df_pt_abn.reset_index()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_pt_abn [[ 'age','gender', 'smokelive','race','pkyr','smokework',\n",
    "                                      'famfather','fammother','anyscr_has_nodule', 'sct_long_dia_sum'\n",
    "                                      ,'sct_long_dia_max', 'sct_long_dia_count','diagcopd'\n",
    "                # Added occupational hazard factors\n",
    "                                      , 'wrkasbe','wrk_total','wrk_or_not'\n",
    "                # Add asthma and smoking factors\n",
    "                                      , 'diagadas','cigsmok','smokeday','Marital_Status']]\n",
    "y = df_pt_abn ['conflc']\n",
    "y = y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "## Random_state is the\"set seed\" in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8694444444444445\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "## Fit a decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "tree2 = DecisionTreeClassifier(max_depth = 4, criterion=\"entropy\").fit(X_train, y_train)\n",
    "print(tree2.score(X_test, y_test))\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot_ng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3b5d87bbdf39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Plotting Decision Tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydot_ng\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot_ng'"
     ]
    }
   ],
   "source": [
    "## Plotting Decision Tree\n",
    "lpy = [item for item in X_train.columns]\n",
    "import pydot_ng as pydot\n",
    "from IPython.display import IFrame\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "with open(\"dt.dot\",\"w\") as dot_data:\n",
    "    export_graphviz(tree2, out_file=dot_data, filled=True, \n",
    "                feature_names = lpy,label = 'all')\n",
    "pydot.graph_from_dot_file(\"dt.dot\").write_png(\"dt.png\")\n",
    "IFrame(\"dt.png\", width = 1000, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work undone: Plotting Feature Importance??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8673611111111111"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "GBDT = GradientBoostingClassifier(learning_rate = .1, max_depth = 4, random_state = 0)\n",
    "gbdt = GBDT.fit(X_train, y_train)\n",
    "gbdt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient boost decision tree classifier [[  52  147]\n",
      " [  44 1197]]\n"
     ]
    }
   ],
   "source": [
    "## Confusion Matrics for Gradient Boosted Decision Tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gbdt_predicted = gbdt.predict(X_test)\n",
    "confusion_gbdt = confusion_matrix(y_test, gbdt_predicted)\n",
    "print('gradient boost decision tree classifier',  confusion_gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Precision, which matters more: 0.54\n",
      "Recall: 0.26\n",
      "F1: 0.35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) \n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, gbdt_predicted)))\n",
    "print('Precision, which matters more: {:.2f}'.format(precision_score(y_test, gbdt_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, gbdt_predicted)))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, gbdt_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.88\n",
      "Accuracy of K-NN classifier on test set: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer dataset\n",
      "Accuracy of Linear SVC classifier on training set: 0.94\n",
      "Accuracy of Linear SVC classifier on test set: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "print('Cancer dataset')\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range = np.logspace(-3, 3, 4)\n",
    "train_scores, test_scores = validation_curve(SVC(), X, y,\n",
    "                                            param_name='gamma',\n",
    "                                            param_range=param_range, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11946b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "#  See:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with SVM')\n",
    "plt.xlabel('$\\gamma$ (gamma)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data is not binary and pos_label is not specified",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5854c823ca08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_score_gbdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfpr_gbdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_gbdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score_gbdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mroc_auc_gbdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_gbdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_gbdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    335\u001b[0m              \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m              np.array_equal(classes, [1]))):\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data is not binary and pos_label is not specified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is not binary and pos_label is not specified"
     ]
    }
   ],
   "source": [
    "## Plotting ROC Curves 这个会报错啊啊啊啊啊啊啊！！！！！\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_score_gbdt = gbdt.decision_function(X_test)\n",
    "fpr_gbdt, tpr_gbdt, _ = roc_curve(y_test, y_score_gbdt)\n",
    "roc_auc_gbdt = auc(fpr_gbdt, tpr_gbdt)\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_gbdt, tpr_gbdt, lw=3, label='GBDT ROC curve (area = {:0.2f})'.format(roc_auc_gbdt))\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Handling of missing values[Done, in PPT]\n",
    "    Answer: discard them, put in notes for \"exclusion criteria\" and add a [workflow] diagram if we publish this         \n",
    "- Add 'sct_ab_desc' in the final data set. Jinglu used this field to be the 'nodule size' in her analysis. \n",
    "- Use other variables: Occupational hazard: Aggregate in sigma function: Have you ever been exposed to a risk at work for lung cancer? 14个标红的变量\n",
    "    # Collaps into one variable if any one of them occur: section 10 有14个标红的    \n",
    "    # asthma, number of cigar smoked, current smoker(cigsmoke)    \n",
    "    # Marital Status: Never Married, Married-like, Seperated/Divorced, N will be decreased by this, which is  fine.\n",
    "- Aggregating variables, select features, boost performance\n",
    "------------------------------------------FINISHED--------------------------------------------------------------\n",
    "- Learn how to write plot and graph. and Plotting feature importance\n",
    "- How can I install Coursera 'Shared Utility'? Ask Tina\n",
    "- Have to have an AUC, god. AUC changes while random state change from 1, 2, and 3\n",
    "- Viz: EDA and Model performance: Nodule size, pack year, risk calculater\n",
    "\n",
    "- Refine model using ensemble method, and other tree methods: Bagging better than boosting, bootstraping: C 5.0, RF. Read on other forms of predictive models, fine 3 are not in the DT family: NB, SVM, glm, NNMF (Non-negative matrix factorization), PCA, and other dimensionality reduction formats that \"keep data intact\"\n",
    "\n",
    "- Plot in benchmark analysis (with other model)\n",
    "\n",
    "- Publish as a new workbook \n",
    "- Write papers (DDL Jan15 done with manuscript, Jan31 application for funding)\n",
    "- Reading for Reponsible conduct research"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Things done after Christmas Break:\n",
    "- Drafted exclusion flowchart\n",
    "- Added in occupational hazard (after collapse), and athsma and smoke number variables, all significant. But occupational hazard becomes less in the decision tree after adding smoke variable\n",
    "- Succesfully beated 86% accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  ## Questions to ask after Christmas Break:\n",
    "1. why we use 51, 52, 61, 62 in \n",
    "2. Is that OK if I don't have small nodules? The answer is no. I have to use sct_ab_desc as indicator of nodule number and sizes. \n",
    "3. How can I use sct_ab_desc as a predictor? One pid has multiple sct_ab_desc. In order to not get duplicated pid, I have to break down the dataset by study year, but how? \n",
    "4. Why is the 'collapsing' step has so many lines of warnings?\n",
    "5. Which criterion should I use for decision tree classifier? gini or enthropy?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Important to notice: 开始没有癌症，不等于eventually没有癌症.They scan for 1-2 yrs and followed up for 8 years, s所以要把studyr＊8 拽出来\n",
    "- 机器学习特征： Age, Race, Packyr, NLST_COPD(?), AbnormalCTdiametersize, AbnormalCTnumberofsuspiciousmasses, (or sct_long_dia in 3rd dataset),Family history, Smoking now, smoke at work, sex, mother, at home, Beat 85% accuracy\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Issues Resolved in the last meeting: \n",
    "- Github\n",
    "- Yes, there are 324 columns in the first file, True n: 1089 + 5290 = 6379 Data pre-processing(This number is acheived): Only give me :1) 6379个人有conflc==1 or 2，是CT scan，2）Add up to generate the sum, and also the largest, just like the LIDC\n",
    "- I have figure out the ratio of cancer vs. Non-cancer\n",
    "- Make sure, the sharing of code works\n",
    "- ## Did Jinglu use max, sum or count??? Which COPD is important?? \n",
    "     Answer: \"Chronic Obstructive Pulmonany Disease\" \n",
    "- 取出table 1, output， 检查一下dataset，missing啊，datatype啊之类的"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Historical 'Undone' before December :\n",
    "- 先Join起来，后两个dataset都有studyr Try to join 3rd dataset\n",
    "- 然后高要求才是，把第二个lc dataset也join进去，我的方案（这个可以问）是不是把最后一年的取出来？？\n",
    "- Continue Michighan and IBM Data Analysis Course\n",
    "- Do I need to make pid the index?\n",
    "- 把几个论文里标亮的地方重新看一遍\n",
    "- Compare logistic Regression??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Historical 'Done' before December\n",
    "\n",
    "- What's the expectation for my work by Sunday? Can we study together\n",
    "    1. [Done]Left-join the 3rd dataset (Abnormal CT Findings)[Done]\n",
    "    2. [Done]Figure out the true number of \"confirmed cancers\" for CT patients (1089, 2058, 2150, ???)[Done]: It is comfirmed to be 1089 Cancer vs 5290 No Cancer\n",
    "    3. [Done]Calculate 4 new columns based on what you did with the LIDC dataset (small nodules, big nodules, total nodules, total size)\n",
    "    Decision tree\n",
    "    Random Forest\n",
    "    C5.0\n",
    "    AUC, evaluate performance, is it above 80%? 85%? What are the top features?(with Importance score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep some distance! Here's about the 3rd dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_lc.head()\n",
    "        #check duplicate pid in df_lc\n",
    "lcid = df_lc[\"pid\"]\n",
    "df_lc[lcid.isin(lcid[lcid.duplicated()])].head(15)\n",
    "Why is pid duplicated here? 1) T0-T7 study yrs, and 2) Multiple cancers\n",
    "## Check how many pid actually exist in this data frame\n",
    "df_lc['pid'].nunique()\n",
    "## Exactly 2058 patient discussed in the last meeting, as compared to 2150 in this dataset\n",
    "df_pt_pd = pd.merge(df_pt, df_lc, how='left', left_on='pid', right_on='pid')\n",
    "df_pt_pd.columns\n",
    "df_pt_pd.dtypes\n",
    "df_pt_pd.shape\n",
    "df_pt_pd.info\n",
    "## Select Useful Columns\n",
    "selected_features  = ['pid','age','cigar','pkyr','smokelive', 'can_scr', 'famfather','fammother']\n",
    "#'lesionsize_y' is not included now\n",
    "df = df_pt_pd[selected_features]\n",
    "df.shape\n",
    "## Check duplication of pid\n",
    "ids = df[\"pid\"]\n",
    "df[ids.isin(ids[ids.duplicated()])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attribute usage:\n",
    "# 100.00% RaceCategory #100.00% Ethnicity\n",
    "#100.00% PackYears\n",
    "#99.99% Age\n",
    "#99.51% FamilyHxFather #99.34% Sex\n",
    "#99.25% FamilyHxMother #98.57% SmokingNowCategory\n",
    "\n",
    "\n",
    "# 100.00% Ethnicity #100.00% PackYears\n",
    "#99.71% Age\n",
    "#97.78% RaceCategory \n",
    "#84.81% FamilyHxMother \n",
    "#80.36% SmokingNowCategory \n",
    "#74.82% SecondSmokeAtHome \n",
    "#72.75% FamilyHxFather #64.44% Sex\n",
    "\n",
    "#MeanDecreaseGini\n",
    "#Age 1335.7826\n",
    "#Sex 201.7961\n",
    "#PackYears 1963.1295\n",
    "#AbnormalCTdiametersize 916.6168\n",
    "#AbnormalCTnumberofsuspiciousmasses 703.6684\n",
    "#AbnormalCTtype   0.0000\n",
    "#RaceCategory 293.6279\n",
    "#EthnicityCategory 104.7826\n",
    "#FamilyHxFather 186.8981\n",
    "#FamilyHxMother 169.8810\n",
    "#SecondSmokeAtHome 219.8933\n",
    "#SecondSmokeAtWork 167.0132\n",
    "#SmokingNowCategory 204.0901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
